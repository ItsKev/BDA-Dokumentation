
@inproceedings{he_physhare:_2017,
	address = {New York, NY, USA},
	series = {{UIST} '17},
	title = {{PhyShare}: {Sharing} {Physical} {Interaction} in {Virtual} {Reality}},
	isbn = {978-1-4503-5419-6},
	shorttitle = {{PhyShare}},
	url = {http://doi.acm.org/10.1145/3131785.3131795},
	doi = {10.1145/3131785.3131795},
	abstract = {We present PhyShare, a new haptic user interface based on actuated robots. Virtual reality has recently been gaining wide adoption, and an effective haptic feedback in these scenarios can strongly support user's sensory in bridging virtual and physical world. Since participants do not directly observe these robotic proxies, we investigate the multiple mappings between physical robots and virtual proxies that can utilize the resources needed to provide a well rounded VR experience. PhyShare bots can act either as directly touchable objects or invisible carriers of physical objects, depending on different scenarios. They also support distributed collaboration, allowing remotely located VR collaborators to share the same physical feedback.},
	urldate = {2019-02-20},
	booktitle = {Adjunct {Publication} of the 30th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {ACM},
	author = {He, Zhenyi and Zhu, Fengyuan and Perlin, Ken},
	year = {2017},
	note = {event-place: Québec City, QC, Canada},
	keywords = {haptic user interfaces, robots, virtual reality},
	pages = {17--19},
	file = {ACM Full Text PDF:C\:\\Users\\kevin\\Zotero\\storage\\3YLYTV5I\\He et al. - 2017 - PhyShare Sharing Physical Interaction in Virtual .pdf:application/pdf}
}

@inproceedings{ouramdane-djerrah_new_2007,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {New} {Model} of {Collaborative} 3D {Interaction} in {Shared} {Virtual} {Environment}},
	isbn = {978-3-540-73107-8},
	abstract = {Recent advances in both Virtual Reality (VR) systems and Computer-Supported Cooperative Work (CSCW) technologies have resulted in the appearance of the Collaborative Virtual Environments (CVEs) systems supporting different forms of collaboration and interaction between users. The collaboration in these systems refers to the simultaneous interaction (collaborative interaction) of multiple users on a virtual object in an immersive or semi-immersive Virtual Environment (VE). However, in some cases, the collaborative interaction is reduced to a simple communication between users. In this paper, we propose a new model of collaborative interaction that supports group interaction in CVEs. Our model defines the functional role and the functional clover of the 3D interaction. This Model is based on group awareness concepts (focus, nimbus and degree of interaction) combined with 3D interaction paradigms (navigation, selection and manipulation). The aim of our model is to manage and control the simultaneous user actions.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction}. {Interaction} {Platforms} and {Techniques}},
	publisher = {Springer Berlin Heidelberg},
	author = {Ouramdane-Djerrah, Nassima and Otmane, Samir and Mallem, Malik},
	editor = {Jacko, Julie A.},
	year = {2007},
	keywords = {virtual reality, 3D interaction, collaborative interaction, collaborative virtual environment},
	pages = {663--672}
}

@article{benford_collaborative_2001,
	title = {Collaborative {Virtual} {Environments}},
	volume = {44},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/379300.379322},
	doi = {10.1145/379300.379322},
	number = {7},
	urldate = {2019-02-20},
	journal = {Commun. ACM},
	author = {Benford, Steve and Greenhalgh, Chris and Rodden, Tom and Pycock, James},
	month = jul,
	year = {2001},
	pages = {79--85},
	file = {ACM Full Text PDF:C\:\\Users\\kevin\\Zotero\\storage\\LEIHI3MN\\Benford et al. - 2001 - Collaborative Virtual Environments.pdf:application/pdf}
}

@inproceedings{boud_virtual_1999,
	title = {Virtual reality and augmented reality as a training tool for assembly tasks},
	doi = {10.1109/IV.1999.781532},
	abstract = {In this paper we investigate whether virtual reality (VR) and augmented reality (AR) offer potential for the training of manual skills, such as for assembly tasks, in comparison to conventional media. We present results from experiments that compare assembly completion times for a number of different conditions. We firstly investigate completion times for a task where participants can study an engineering drawing and an assembly plan and then conduct the task. We then investigate the task under various VR conditions and context-free AR. We discuss the relative advantages and limitations of using VR and AR as training media for investigating assembly operations, and we present the results of our experimental work.},
	booktitle = {1999 {IEEE} {International} {Conference} on {Information} {Visualization} ({Cat}. {No}. {PR}00210)},
	author = {Boud, A. C. and Haniff, D. J. and Baber, C. and Steiner, S. J.},
	month = jul,
	year = {1999},
	keywords = {virtual reality, augmented reality, assembling, Assembly, assembly completion times, assembly plan, assembly tasks, Augmented reality, computer based training, Computer displays, Computer graphics, context-free augmented reality, engineering drawing, engineering graphics, Head, Humans, manual skills training, Mechanical engineering, Mirrors, Pulp manufacturing, training media, Virtual reality},
	pages = {32--36},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\kevin\\Zotero\\storage\\BJ2PM9XN\\781532.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\kevin\\Zotero\\storage\\A6K3PR75\\Boud et al. - 1999 - Virtual reality and augmented reality as a trainin.pdf:application/pdf}
}

@misc{noauthor_virtual_nodate,
	title = {Virtual reality in engineering education: {The} future of creative learning - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore.ieee.org/abstract/document/5773223},
	urldate = {2019-02-20},
	file = {Virtual reality in engineering education\: The future of creative learning - IEEE Conference Publication:C\:\\Users\\kevin\\Zotero\\storage\\3HXH25J8\\5773223.html:text/html}
}

@inproceedings{schild_applying_2018,
	title = {Applying {Multi}-{User} {Virtual} {Reality} to {Collaborative} {Medical} {Training}},
	doi = {10.1109/VR.2018.8446160},
	abstract = {We present a multi-user virtual reality (VR) setup that aims at providing novel training tools for paramedics that enhances current learning methods. The hardware setup consists of a two-user full-scale VR environment with head-mounted displays for two interactive trainees and one additional desktop pc for one trainer participant. The software provides a connected multi-user environment, showcasing a paramedic emergency simulation with focus on anaphylactic shock, a representative scenario for critical medical cases that happen too rare to eventually occur within a regular curricular term of vocational training. The prototype offers hands-on experience on multi-user VR in an applied scenario, generating discussion around current state and future development concerning three important research areas: (a) user navigation, (b) interaction, (c) level of visual abstraction, and (d) level of task abstraction.},
	booktitle = {2018 {IEEE} {Conference} on {Virtual} {Reality} and 3D {User} {Interfaces} ({VR})},
	author = {Schild, J. and Misztal, S. and Roth, B. and Flock, L. and Luiz, T. and Lerner, D. and Herkersdorf, M. and Weaner, K. and Neuberaer, M. and Franke, A. and Kemp, C. and Pranqhofer, J. and Seele, S. and Buhler, H. and Herpers, R.},
	month = mar,
	year = {2018},
	keywords = {Virtual reality, Games, medical training, multiuser VR, serious games, Software, Solid modeling, Task analysis, Vocational training},
	pages = {775--776},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\kevin\\Zotero\\storage\\KN4XYS4I\\8446160.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\kevin\\Zotero\\storage\\EGHIWUJ6\\Schild et al. - 2018 - Applying Multi-User Virtual Reality to Collaborati.pdf:application/pdf}
}

@inproceedings{carlsson_dive_1993,
	title = {{DIVE} {A} multi-user virtual reality system},
	doi = {10.1109/VRAIS.1993.380753},
	abstract = {The Distributed Interactive Virtual Environment (DIVE) is a heterogeneous distributed virtual reality system based on UNIX and Internet networking protocols. Each participating process has a copy of a replicated database and changes are propagated to the other processes with reliable multicast protocols. DIVE provides a dynamic virtual environment where applications and users can enter and leave the environment on demand. Several user-related abstractions have been introduced to ease the task of application and user interface construction.{\textless}{\textless}ETX{\textgreater}{\textgreater}},
	booktitle = {Proceedings of {IEEE} {Virtual} {Reality} {Annual} {International} {Symposium}},
	author = {Carlsson, C. and Hagsand, O.},
	month = sep,
	year = {1993},
	keywords = {virtual reality, Virtual reality, application generation, Application software, Collaboration, Computer science, Databases, Distributed Interactive Virtual Environment, DIVE, dynamic virtual environment, graphical user interfaces, groupware, heterogeneous distributed virtual reality system, interactive systems, Internet, Internet networking protocols, IP networks, multi-access systems, multi-user virtual reality system, Multicast protocols, Navigation, reliable multicast protocols, replicated database, replicated databases, transport protocols, Unix, UNIX, user interface construction, User interfaces, user-related abstractions, Virtual environment},
	pages = {394--400},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\kevin\\Zotero\\storage\\UP9SQ4FP\\380753.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\kevin\\Zotero\\storage\\6F2C398A\\Carlsson und Hagsand - 1993 - DIVE A multi-user virtual reality system.pdf:application/pdf}
}

@misc{noauthor_pdf_nodate,
	title = {({PDF}) {Real} {Virtuality}: {A} {Multi}-{User} {Immersive} {Platform} {Connecting} {Real} and {Virtual} {Worlds}},
	shorttitle = {({PDF}) {Real} {Virtuality}},
	url = {https://www.researchgate.net/publication/293313811_Real_Virtuality_A_Multi-User_Immersive_Platform_Connecting_Real_and_Virtual_Worlds},
	abstract = {PDF {\textbar} Real Virtuality is a multiuser immersive platform combining motion capture with virtual reality (VR) headsets: users can freely move within the physical space while virtually visiting a virtual world and interacting with 3D objects or other users using the sense of touch.},
	language = {en},
	urldate = {2019-02-20},
	journal = {ResearchGate},
	doi = {http://dx.doi.org/10.1145/2927929.2927945},
	file = {Snapshot:C\:\\Users\\kevin\\Zotero\\storage\\EI79PQB4\\293313811_Real_Virtuality_A_Multi-User_Immersive_Platform_Connecting_Real_and_Virtual_Worlds.html:text/html}
}

@inproceedings{podkosova_immersivedeck:_2016,
	address = {Greenville, SC, USA},
	title = {Immersivedeck: a large-scale wireless {VR} system for multiple users},
	isbn = {978-1-5090-4275-3},
	shorttitle = {Immersivedeck},
	url = {http://ieeexplore.ieee.org/document/7551581/},
	doi = {10.1109/SEARIS.2016.7551581},
	abstract = {We present preliminary results of work on a low-cost multi-user immersive Virtual Reality system that enables collaborative experiences in large virtual environments. In the proposed setup at least three users can walk and interact freely and untethered in a 200 m2 area. The required equipment is worn on the body and rendering is performed locally on each user to minimize latency. Inside-out optical head tracking is coupled with a low-cost motion capture suit to track the full body and the head. Movements of users, 3D interactions and the positions of selected real world objects are distributed over a wireless network in a server-client architecture. As a result, users see the effect of their interactions with objects and other users in real time. We describe the architecture of our implemented proof-of-concept system.},
	language = {en},
	urldate = {2019-02-20},
	booktitle = {2016 {IEEE} 9th {Workshop} on {Software} {Engineering} and {Architectures} for {Realtime} {Interactive} {Systems} ({SEARIS})},
	publisher = {IEEE},
	author = {Podkosova, Iana and Vasylevska, Khrystyna and Schoenauer, Christian and Vonach, Emanuel and Fikar, Peter and Bronederk, Elisabeth and Kaufmann, Hannes},
	month = mar,
	year = {2016},
	pages = {1--7},
	file = {Podkosova et al. - 2016 - Immersivedeck a large-scale wireless VR system fo.pdf:C\:\\Users\\kevin\\Zotero\\storage\\CPUEKPLN\\Podkosova et al. - 2016 - Immersivedeck a large-scale wireless VR system fo.pdf:application/pdf}
}

@phdthesis{chen_collaboration_2015,
	type = {phdthesis},
	title = {Collaboration in {Multi}-user {Immersive} {Virtual} {Environment}},
	url = {https://tel.archives-ouvertes.fr/tel-01340364/document},
	abstract = {Immersive virtual environment can be used to bring both geographically distributed and co-located users to the same virtual place for collaboration. Compared to remote situations, co-located users collaborate in the same virtual world on top of a shared physical workspace. This collocation allows direct user communication and interaction without computer mediation which facilitates collaborative work. With the development of multi-user display and tracking technology, classical projection-based immersive setups (e.g. CAVE) can now support group immersion for co-located users by offering individual stereoscopic views without visual distortion. In this context, the coexistence of information from the virtual and real world, especially when users do not share a common spatial reference frame, provides users with a new kind of perceptual and cognitive experience. We are interested in how users perceive and communicate with each other to achieve a shared context for collaboration, and how we can broaden supported collaborative scenarios with more flexible viewpoint control.This PhD thesis mainly addresses perceptual and cohabitation issues that we identified in the aim of supporting safe and efficient co-located collaboration in immersive virtual environment. First, we conducted a case study to examine how perceptual conflicts would alter user communication and task performance. Second, we concentrated on the design and evaluation of appropriate navigation paradigms to allow individual virtual navigation while solving cohabitation problems in a shared limited physical workspace. At last, based on the results of previous studies, we designed a generic dynamic navigation model which integrates constrains from the physical workspace and also the virtual world to enable co-located collaboration in multi-user immersive systems.},
	language = {en},
	urldate = {2019-02-20},
	school = {Université Paris-Saclay},
	author = {Chen, Weiya},
	month = dec,
	year = {2015},
	file = {Full Text PDF:C\:\\Users\\kevin\\Zotero\\storage\\ULAQCBWT\\Chen - 2015 - Collaboration in Multi-user Immersive Virtual Envi.pdf:application/pdf;Snapshot:C\:\\Users\\kevin\\Zotero\\storage\\2WKMVHZ9\\tel-01340364.html:text/html}
}

@inproceedings{martin_reconfigurable_2011,
	title = {A reconfigurable architecture for multimodal and collaborative interactions in {Virtual} {Environments}},
	doi = {10.1109/3DUI.2011.5759210},
	abstract = {Many studies have been carried out on multimodal and collaborative systems in VR. Although these two aspects are usually studied separately, they share interesting similarities. This paper focuses on the reconfigurable aspect and the implementation of a multimodal and collaborative supervisor for Virtual Environments (VEs). The aim of this supervisor is to ensure the merge of pieces of information from VR devices in order to control immersive multi-user applications through the main communication and sensorimotor channels of humans. Beyond the architectural aspect, we give indications on the modularity and the genericity of our system, implemented in C++, which could be embedded into different VR platforms. Moreover, its XML-based configuration system allows it to be easily applicable to many different contexts. The reconfigurable features are then illustrated via two scenarios: a cognitive oriented assembly task with single user multimodal interactions, and an industrial assembly task with multimodal and collaborative interactions in a co-located multi-user environment.},
	booktitle = {2011 {IEEE} {Symposium} on 3D {User} {Interfaces} (3DUI)},
	author = {Martin, P. and Bourdot, P. and Touraine, D.},
	month = mar,
	year = {2011},
	keywords = {virtual reality, Collaboration, groupware, Virtual environment, C++, C++ language, cognitive oriented assembly task, collaborative interactions, configuration management, Corporate acquisitions, H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems—Artificial, augmented, and virtual realities, H.5.2 [Information Interfaces and Presentation]: MUser Interfaces—User interface management systems, Haptic interfaces, I.3.6 [Computer Graphics]: Methodology and Techniques—Interaction techniques, industrial assembly, multimodal interactions, multiuser environment, reconfigurable architecture, reconfigurable architectures, sensorimotor channels, Speech recognition, USA Councils, virtual environments, VR devices, XML, XML-based configuration system},
	pages = {11--14},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\kevin\\Zotero\\storage\\J8WRECKX\\5759210.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\kevin\\Zotero\\storage\\IHTXKAJI\\Martin et al. - 2011 - A reconfigurable architecture for multimodal and c.pdf:application/pdf}
}

@inproceedings{pinho_cooperative_2002,
	address = {New York, NY, USA},
	series = {{VRST} '02},
	title = {Cooperative {Object} {Manipulation} in {Immersive} {Virtual} {Environments}: {Framework} and {Techniques}},
	isbn = {978-1-58113-530-5},
	shorttitle = {Cooperative {Object} {Manipulation} in {Immersive} {Virtual} {Environments}},
	url = {http://doi.acm.org/10.1145/585740.585769},
	doi = {10.1145/585740.585769},
	abstract = {Cooperative manipulation refers to the simultaneous manipulation of a virtual object by multiple users in an immersive virtual environment. This paper describes a framework supporting the development of collaborative manipulation techniques, and example techniques we have tested within this framework. We describe the modeling of cooperative interaction techniques, methods of combining simultaneous user actions, and the awareness tools used to provide the necessary knowledge of partner activities during the cooperative interaction process. Our framework is based on a Collaborative Metaphor concept that defines rules to combine user interaction techniques. The combination is based on the separation of degrees of freedom between two users. Finally, we present novel combinations of two interaction techniques (Simple Virtual Hand and Ray-casting).},
	urldate = {2019-02-20},
	booktitle = {Proceedings of the {ACM} {Symposium} on {Virtual} {Reality} {Software} and {Technology}},
	publisher = {ACM},
	author = {Pinho, Márcio S. and Bowman, Doug A. and Freitas, Carla M.D.S.},
	year = {2002},
	note = {event-place: Hong Kong, China},
	keywords = {cooperative interaction, interaction in virtual environments},
	pages = {171--178},
	file = {ACM Full Text PDF:C\:\\Users\\kevin\\Zotero\\storage\\R74D8DYD\\Pinho et al. - 2002 - Cooperative Object Manipulation in Immersive Virtu.pdf:application/pdf}
}

@article{ruddle_symmetric_2002,
	title = {Symmetric and {Asymmetric} {Action} {Integration} {During} {Cooperative} {Object} {Manipulation} in {Virtual} {Environments}},
	volume = {9},
	issn = {1073-0516},
	url = {http://doi.acm.org/10.1145/586081.586084},
	doi = {10.1145/586081.586084},
	abstract = {Cooperation between multiple users in a virtual environment (VE) can take place at one of three levels. These are defined as where users can perceive each other (Level 1), individually change the scene (Level 2), or simultaneously act on and manipulate the same object (Level 3). Despite representing the highest level of cooperation, multiuser object manipulation has rarely been studied. This paper describes a behavioral experiment in which the piano movers' problem (maneuvering a large object through a restricted space) was used to investigate object manipulation by pairs of participants in a VE. Participants' interactions with the object were integrated together either symmetrically or asymmetrically. The former only allowed the common component of participants' actions to take place, but the latter used the mean. Symmetric action integration was superior for sections of the task when both participants had to perform similar actions, but if participants had to move in different ways (e.g., one maneuvering him/herself through a narrow opening while the other traveled down a wide corridor) then asymmetric integration was superior. With both forms of integration, the extent to which participants coordinated their actions was poor and this led to a substantial cooperation overhead (the reduction in performance caused by having to cooperate with another person).},
	number = {4},
	urldate = {2019-02-20},
	journal = {ACM Trans. Comput.-Hum. Interact.},
	author = {Ruddle, Roy A. and Savage, Justin C. D. and Jones, Dylan M.},
	month = dec,
	year = {2002},
	keywords = {object manipulation, piano movers' problem, rules of interaction, Virtual environments},
	pages = {285--308},
	file = {ACM Full Text PDF:C\:\\Users\\kevin\\Zotero\\storage\\8YFF3XE3\\Ruddle et al. - 2002 - Symmetric and Asymmetric Action Integration During.pdf:application/pdf}
}

@article{magnenat-thalmann_interactive_2015,
	title = {Interactive {Virtual} {Humans} in {Real}-{Time} {Virtual} {Environment}},
	volume = {06},
	url = {https://hal.archives-ouvertes.fr/hal-01530607},
	abstract = {In this paper, we will present an overview of existing research in the vast area of IVH systems. We will also present our ongoing work on improving the expressive capabilities of IVHs. Because of the complexity of interaction, a high level of control is required over the face and body motions of the virtual humans. In order to achieve this, current approaches try to generate face and body motions from a high-level description. Although this indeed allows for a precise control over the movement of the virtual human, it is difficult to generate a natural-looking motion from such a high-level description. Another problem that arises when animating IVHs is that motions are not generated all the time. Therefore a flexible animation scheme is required that ensures a natural posture even when no animation is playing. We will present MIRAnim, our animation engine, which uses a combination of motion synthesis from motion capture and a statistical analysis of prerecorded motion clips. As opposed to existing approaches that create new motions with limited flexibility, our model adapts existing motions, by automatically adding dependent joint motions. This renders the animation more natural, but since our model does not impose any conditions on the input motion, it can be linked easily with existing gesture synthesis techniques for IVHs. Because we use a linear representation for joint orientations, blending and interpolation is done very efficiently, resulting in an animation engine especially suitable for real-time applications},
	number = {3},
	urldate = {2019-02-21},
	journal = {International Journal of Virtual Reality},
	author = {Magnenat-Thalmann, Nadia and Egges, Arjan},
	month = nov,
	year = {2015},
	keywords = {Interactive Virtual Humans, Motion Synthesis, Real-time Animation},
	pages = {15--24},
	file = {HAL Snapshot:C\:\\Users\\kevin\\Zotero\\storage\\YN8AYZ6Q\\hal-01530607.html:text/html}
}

@article{seth_virtual_2011,
	title = {Virtual reality for assembly methods prototyping: a review},
	volume = {15},
	issn = {1434-9957},
	shorttitle = {Virtual reality for assembly methods prototyping},
	url = {https://doi.org/10.1007/s10055-009-0153-y},
	doi = {10.1007/s10055-009-0153-y},
	abstract = {Assembly planning and evaluation is an important component of the product design process in which details about how parts of a new product will be put together are formalized. A well designed assembly process should take into account various factors such as optimum assembly time and sequence, tooling and fixture requirements, ergonomics, operator safety, and accessibility, among others. Existing computer-based tools to support virtual assembly either concentrate solely on representation of the geometry of parts and fixtures and evaluation of clearances and tolerances or use simulated human mannequins to approximate human interaction in the assembly process. Virtual reality technology has the potential to support integration of natural human motions into the computer aided assembly planning environment (Ritchie et al. in Proc I MECH E Part B J Eng 213(5):461–474, 1999). This would allow evaluations of an assembler’s ability to manipulate and assemble parts and result in reduced time and cost for product design. This paper provides a review of the research in virtual assembly and categorizes the different approaches. Finally, critical requirements and directions for future research are presented.},
	language = {en},
	number = {1},
	urldate = {2019-02-25},
	journal = {Virtual Reality},
	author = {Seth, Abhishek and Vance, Judy M. and Oliver, James H.},
	month = mar,
	year = {2011},
	keywords = {Virtual reality, Collision detection, Constraint-based modeling, Haptics, Human–computer interaction, Physics-based modeling, Virtual assembly},
	pages = {5--20},
	file = {Springer Full Text PDF:C\:\\Users\\kevin\\Zotero\\storage\\J6UUW5CU\\Seth et al. - 2011 - Virtual reality for assembly methods prototyping .pdf:application/pdf}
}

@book{sherman_understanding_2018,
	title = {Understanding {Virtual} {Reality}: {Interface}, {Application}, and {Design}},
	isbn = {978-0-12-801038-9},
	shorttitle = {Understanding {Virtual} {Reality}},
	abstract = {Understanding Virtual Reality: Interface, Application, and Design, Second Edition arrives at a time when the technologies behind virtual reality have advanced dramatically. The book helps users take advantage of the ways they can identify and prepare for the applications of VR in their field. By approaching VR as a communications medium, the authors have created a resource that will remain relevant even as underlying technologies evolve. Included are a history of VR, systems currently in use, the application of VR, and the many issues that arise in application design and implementation, including hardware requirements, system integration, interaction techniques and usability. Features substantive, illuminating coverage designed for technical or business readers and the classroomExamines VR's constituent technologies, drawn from visualization, representation, graphics, human-computer interaction and other fieldsProvides (via a companion website) additional case studies, tutorials, instructional materials, and a link to an open-source VR programming systemIncludes updated perception material and new sections on game engines, optical tracking, VR visual interface software, and a new glossary with pictures},
	language = {en},
	publisher = {Morgan Kaufmann},
	author = {Sherman, William R. and Craig, Alan B.},
	month = nov,
	year = {2018},
	keywords = {Computers / Virtual Worlds}
}

@inproceedings{simeone_substitutional_2015,
	address = {New York, NY, USA},
	series = {{CHI} '15},
	title = {Substitutional {Reality}: {Using} the {Physical} {Environment} to {Design} {Virtual} {Reality} {Experiences}},
	isbn = {978-1-4503-3145-6},
	shorttitle = {Substitutional {Reality}},
	url = {http://doi.acm.org/10.1145/2702123.2702389},
	doi = {10.1145/2702123.2702389},
	abstract = {Experiencing Virtual Reality in domestic and other uncontrolled settings is challenging due to the presence of physical objects and furniture that are not usually defined in the Virtual Environment. To address this challenge, we explore the concept of Substitutional Reality in the context of Virtual Reality: a class of Virtual Environments where every physical object surrounding a user is paired, with some degree of discrepancy, to a virtual counterpart. We present a model of potential substitutions and validate it in two user studies. In the first study we investigated factors that affect participants' suspension of disbelief and ease of use. We systematically altered the virtual representation of a physical object and recorded responses from 20 participants. The second study investigated users' levels of engagement as the physical proxy for a virtual object varied. From the results, we derive a set of guidelines for the design of future Substitutional Reality experiences.},
	urldate = {2019-02-28},
	booktitle = {Proceedings of the 33rd {Annual} {ACM} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Simeone, Adalberto L. and Velloso, Eduardo and Gellersen, Hans},
	year = {2015},
	note = {event-place: Seoul, Republic of Korea},
	keywords = {virtual reality, passive haptics, substitutional reality},
	pages = {3307--3316},
	file = {ACM Full Text PDF:C\:\\Users\\kevin\\Zotero\\storage\\F8IADLB2\\Simeone et al. - 2015 - Substitutional Reality Using the Physical Environ.pdf:application/pdf}
}

@inproceedings{de_oliveira_usage_2017,
	address = {Banff, AB},
	title = {Usage of tactile feedback to assist cooperative object manipulations in virtual environments},
	isbn = {978-1-5386-1645-1},
	url = {http://ieeexplore.ieee.org/document/8122976/},
	doi = {10.1109/SMC.2017.8122976},
	abstract = {This study evaluates the usage of tactile feedback to aid cooperative object manipulation using the SkeweR technique. This technique is based on the use of crushing points, where the users grab the object for the first time, to simultaneously move/rotate an object. Once the user keeps his hand positioned on the crushing point, during the object manipulation, the interaction becomes more natural, in the sense that it is more similar to the real process. However, due to the lack of any physical constraint to the users’ movements, it is often noticed that the user’s hand moves apart from the crushing point during the interaction. To solve this problem, this work proposes the usage of tactile feedback to inform the user about the distance of his hand from the crushing point. The tactile feedback is provided by a vibration micromotor attached to the users’ thumb. To validate our method, we ran a user study based on the 3D manipulation of a virtual object, which has to be translated and rotated through a virtual path along a virtual wire, from the beginning to the end of it. During the interaction, users manipulate a 3DOF position tracker and should keep this tracker at the same position of the crushing point. During the trials, the participants used three modalities of interaction: without any feedback, with a visual feedback and with tactile feedback. Results showed that the users kept the tracker closer to the crushing point when using tactile feedback.},
	language = {en},
	urldate = {2019-02-28},
	booktitle = {2017 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({SMC})},
	publisher = {IEEE},
	author = {de Oliveira, Thomas Volpato and Pinho, Marcio Sarroglia},
	month = oct,
	year = {2017},
	pages = {2367--2372},
	file = {de Oliveira und Pinho - 2017 - Usage of tactile feedback to assist cooperative ob.pdf:C\:\\Users\\kevin\\Zotero\\storage\\YW66PNL4\\de Oliveira und Pinho - 2017 - Usage of tactile feedback to assist cooperative ob.pdf:application/pdf}
}

@article{seth_virtual_2011-1,
	title = {Virtual reality for assembly methods prototyping: a review},
	volume = {15},
	issn = {1434-9957},
	shorttitle = {Virtual reality for assembly methods prototyping},
	url = {https://doi.org/10.1007/s10055-009-0153-y},
	doi = {10.1007/s10055-009-0153-y},
	abstract = {Assembly planning and evaluation is an important component of the product design process in which details about how parts of a new product will be put together are formalized. A well designed assembly process should take into account various factors such as optimum assembly time and sequence, tooling and fixture requirements, ergonomics, operator safety, and accessibility, among others. Existing computer-based tools to support virtual assembly either concentrate solely on representation of the geometry of parts and fixtures and evaluation of clearances and tolerances or use simulated human mannequins to approximate human interaction in the assembly process. Virtual reality technology has the potential to support integration of natural human motions into the computer aided assembly planning environment (Ritchie et al. in Proc I MECH E Part B J Eng 213(5):461–474, 1999). This would allow evaluations of an assembler’s ability to manipulate and assemble parts and result in reduced time and cost for product design. This paper provides a review of the research in virtual assembly and categorizes the different approaches. Finally, critical requirements and directions for future research are presented.},
	language = {en},
	number = {1},
	urldate = {2019-03-12},
	journal = {Virtual Reality},
	author = {Seth, Abhishek and Vance, Judy M. and Oliver, James H.},
	month = mar,
	year = {2011},
	keywords = {Virtual reality, Collision detection, Constraint-based modeling, Haptics, Human–computer interaction, Physics-based modeling, Virtual assembly},
	pages = {5--20},
	file = {Springer Full Text PDF:C\:\\Users\\kevin\\Zotero\\storage\\CBTWVA2C\\Seth et al. - 2011 - Virtual reality for assembly methods prototyping .pdf:application/pdf}
}

@article{yang_virtual_2007,
	title = {Virtual assembly technologies based on constraint and {DOF} analysis},
	volume = {23},
	issn = {0736-5845},
	url = {http://www.sciencedirect.com/science/article/pii/S0736584506000755},
	doi = {10.1016/j.rcim.2006.05.008},
	abstract = {Assembly constraint is an important factor for product assembly. Constraint is not only necessary for accurate locating of parts but also an important way to realize assembly operation process in virtual assembly (VA) system. In this study, to realize VA reasonably, uniform representations of assembly constraint, equivalent relation between constraint and degree of freedom (DOF), and movable DOF reduction are defined. Several algorithms including based-constraint assembly relation recognition, assembly parts location solving under free and constrained spaces, and movement navigation are developed. These algorithms have been applied to integration virtual assembly environment (IVAE) software system. An example is illustrated, and the results prove that the algorithms provide a good support for the constraint treatments in IVAE and the high consistence between virtual and real assembly process is realized.},
	number = {4},
	urldate = {2019-03-12},
	journal = {Robotics and Computer-Integrated Manufacturing},
	author = {Yang, Run Dang and Fan, XiuMin and Wu, DianLiang and Yan, JuanQi},
	month = aug,
	year = {2007},
	keywords = {Virtual reality, Virtual assembly, Constraint recognition, DOF reduction, Location solving, Motion navigation},
	pages = {447--456},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\kevin\\Zotero\\storage\\3SDG4NA9\\Yang et al. - 2007 - Virtual assembly technologies based on constraint .pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\kevin\\Zotero\\storage\\NBSGNJPB\\S0736584506000755.html:text/html}
}

@article{wang_methods_2003,
	title = {Methods and {Algorithms} for {Constraint}-based {Virtual} {Assembly}},
	volume = {6},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-003-0106-9},
	doi = {10.1007/s10055-003-0106-9},
	abstract = {Constraint-based simulation is a fundamental concept used for assembly in a virtual environment. The constraints (axial, planer, etc.) are extracted from the assembly models in the CAD system and are simulated during the virtual assembly operation to represent the real world operations. In this paper, we present the analysis of ‘combinations’ and ‘order of application’ of axial and planar constraints used in assembly. Methods and algorithms for checking and applying the constraints in the assembly operation are provided. An object-oriented model for managing these constraints in the assembly operation is discussed.},
	language = {en},
	number = {4},
	urldate = {2019-03-12},
	journal = {Virtual Reality},
	author = {Wang, Y. and Jayaram, U. and Jayaram, S. and Imtiyaz, S.},
	month = aug,
	year = {2003},
	keywords = {Assembly Constraints, Constrained Motion Simulation, Virtual Assembly},
	pages = {229--243},
	file = {Springer Full Text PDF:C\:\\Users\\kevin\\Zotero\\storage\\YSUUX8RL\\Wang et al. - 2003 - Methods and Algorithms for Constraint-based Virtua.pdf:application/pdf}
}

@article{tching_interactive_2010,
	title = {Interactive simulation of {CAD} models assemblies using virtual constraint guidance},
	volume = {4},
	issn = {1955-2505},
	url = {https://doi.org/10.1007/s12008-010-0091-7},
	doi = {10.1007/s12008-010-0091-7},
	abstract = {In the context of virtual reality (VR) and of computed aided design (CAD), haptic simulations are used to perform assembly tasks between 3D objects. To ensure the good assembly of those objects, we propose a new method of interactive assembly that uses both kinematic constraints and guiding virtual fixtures. Modelling a haptic assembly task as a combination of mechanical joints, we focus on the guidance of objects and on the activation cues of kinematic constraints in physical simulation. In this article, we first outline the difficulties related to the haptic-assembly of CAD objects in VR simulation. Introducing the virtual constraint guidance (VCG), we present a new method for haptic guidance that decomposes a task in two independent steps: a guiding step which use geometries as virtual fixtures to position objects, and a functional step which use kinematic constraints to perform the assembly task. We finally present a complete application of our method on a peg-in-hole insertion task.},
	language = {en},
	number = {2},
	urldate = {2019-03-12},
	journal = {International Journal on Interactive Design and Manufacturing (IJIDeM)},
	author = {Tching, Loïc and Dumont, Georges and Perret, Jérôme},
	month = may,
	year = {2010},
	keywords = {Virtual reality, 3D assembling task, Geometric guidance, Haptic simulation, Kinematic constraint, Peg-in-hole, Virtual fixture},
	pages = {95--102},
	file = {Springer Full Text PDF:C\:\\Users\\kevin\\Zotero\\storage\\5I3LG34D\\Tching et al. - 2010 - Interactive simulation of CAD models assemblies us.pdf:application/pdf}
}

@article{tirumali_vade:_1999,
	title = {{VADE}: a {Virtual} {Assembly} {Design} {Environment}},
	volume = {19},
	issn = {0272-1716},
	shorttitle = {{VADE}},
	doi = {10.1109/38.799739},
	abstract = {Virtual Assembly Design Environment (VADE) resulted from a research and development project started in 1995, sponsored by the National Institute of Standards and Technology (NIST). The main purpose of this project was to explore the potential and the technical challenges in using VR technologies for design and manufacturing by creating a VE for assembly planning and evaluation. In this article, we describe the overall system, the important features, and examples of using VADE. We also discuss the benefits and limitations of virtual assembly systems. In addition, we compare virtual assembly and automated assembly planning systems.},
	number = {6},
	journal = {IEEE Computer Graphics and Applications},
	author = {Tirumali, {and} H. and Lyons, K. and Hart, P.},
	month = nov,
	year = {1999},
	keywords = {virtual reality, engineering graphics, Virtual reality, assembly planning, Assembly systems, automated assembly planning systems, computer aided production planning, Data gloves, Design automation, Design engineering, manufacturing, NIST, Process planning, Prototypes, Time series analysis, Virtual Assembly Design Environment, Virtual prototyping},
	pages = {44--50},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\kevin\\Zotero\\storage\\7JSKL2L3\\799739.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\kevin\\Zotero\\storage\\J7WR2WMV\\and et al. - 1999 - VADE a Virtual Assembly Design Environment.pdf:application/pdf}
}

@misc{noauthor_steamvr_2019,
	title = {{SteamVR} {Plugin}},
	url = {https://assetstore.unity.com/packages/tools/integration/steamvr-plugin-32647},
	urldate = {2019-05-29},
	year = {2019},
	file = {SteamVR Plugin - Asset Store:C\:\\Users\\kevin\\Zotero\\storage\\TNDWHTAX\\steamvr-plugin-32647.html:text/html}
}

@misc{noauthor_unity_2019,
	title = {Unity},
	url = {https://unity.com/},
	urldate = {2019-05-29},
	year = {2019},
	file = {Unity:C\:\\Users\\kevin\\Zotero\\storage\\T9Q5QPSP\\unity.com.html:text/html}
}

@misc{noauthor_vive_2019,
	title = {{VIVE}™},
	url = {https://www.vive.com/eu/},
	urldate = {2019-05-29},
	year = {2019},
	file = {VIVE™ | Discover Virtual Reality Beyond Imagination:C\:\\Users\\kevin\\Zotero\\storage\\NHXHHDR3\\eu.html:text/html}
}

@misc{noauthor_photon_2019,
	title = {Photon {Engine}},
	url = {https://www.photonengine.com/},
	urldate = {2019-05-29},
	year = {2019},
	file = {Multiplayer Game Development Made Easy | Photon Engine:C\:\\Users\\kevin\\Zotero\\storage\\S2P2F7ST\\www.photonengine.com.html:text/html}
}

@misc{noauthor_photon_2019-1,
	title = {Photon {Voice} 2},
	url = {https://assetstore.unity.com/packages/tools/audio/photon-voice-2-130518},
	urldate = {2019-05-30},
	year = {2019},
	file = {Photon Voice 2 - Asset Store:C\:\\Users\\kevin\\Zotero\\storage\\KDK475DE\\photon-voice-2-130518.html:text/html}
}

@misc{noauthor_lab_2019,
	title = {The {Lab}},
	url = {https://store.steampowered.com/app/450390/The_Lab/},
	urldate = {2019-05-30},
	year = {2019},
	file = {The Lab on Steam:C\:\\Users\\kevin\\Zotero\\storage\\U3UIBQ5Y\\The_Lab.html:text/html}
}